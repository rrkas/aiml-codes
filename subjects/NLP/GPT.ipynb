{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b8173a4",
   "metadata": {},
   "source": [
    "## OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd509946",
   "metadata": {},
   "source": [
    "In the context of GPT (Generative Pre-trained Transformer) models provided by OpenAI, the term \"temperature\" refers to a parameter that controls the randomness of the model's output. It influences the likelihood of the model choosing different words when generating text.\n",
    "\n",
    "How Temperature Affects Output\n",
    "Low Temperature (e.g., 0.2):\n",
    "\n",
    "The model becomes more deterministic and conservative.\n",
    "It tends to produce more predictable and repetitive responses, as it chooses words with higher probabilities.\n",
    "Useful for tasks requiring precise and specific answers.\n",
    "High Temperature (e.g., 0.8):\n",
    "\n",
    "The model's output becomes more random and diverse.\n",
    "It is more likely to take risks and generate creative or varied responses, as it allows for words with lower probabilities.\n",
    "Useful for creative writing or when diversity in responses is desired."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fc2fd0",
   "metadata": {},
   "source": [
    "In the context of GPT models, \"tokens\" refer to pieces of text that the model processes. These can be as short as one character or as long as one word, depending on the language and context. The \"max tokens\" parameter in OpenAI's API controls the maximum length of the generated output.\n",
    "\n",
    "Tokens and Model Limits\n",
    "Understanding Tokens:\n",
    "\n",
    "Tokens can be whole words or just parts of words. For instance, the word \"hello\" is one token, while \"internationalization\" might be split into multiple tokens.\n",
    "The total number of tokens is the sum of the input tokens and the output tokens. For example, if you input a prompt that is 20 tokens long and request a response with a maximum of 30 tokens, the total token count is 50.\n",
    "Max Tokens Parameter:\n",
    "\n",
    "The max_tokens parameter specifies the maximum number of tokens that the model is allowed to generate in its response.\n",
    "This parameter helps control the length of the output and ensures the response fits within certain limits.\n",
    "Token Limits by Model:\n",
    "\n",
    "Different models have different maximum token limits. As of my last update, the following are typical limits for various OpenAI models:\n",
    "GPT-3 (davinci): Up to 4097 tokens (combined input and output).\n",
    "GPT-3.5-turbo and GPT-4: Up to 8192 tokens (combined input and output).\n",
    "Example Usage\n",
    "Here’s how you can set the max_tokens parameter in a request to OpenAI’s API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc8e1312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from gpt import GPT, Example\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a23bcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"API-KEY\"\n",
    "\n",
    "gpt = GPT(engine=\"davinci\", temperature=0.5, max_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a746f64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"Gender\": [\"boy\", \"boy\"]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
